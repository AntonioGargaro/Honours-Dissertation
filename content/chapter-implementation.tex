% !TEX root = ../thesis-example.tex
%
\chapter{Implementation}
\label{sec:implementation}
%\vspace{-4em}{

The development of this web app can be split into three significant processes. The information and communications process which instated the backbone of controlling, gathering and communicating data. The bot process which is derived of three concurrent \textit{`actor'} models to gather and process Binance exchange data, then forwarding the generated signal results to the correct user. Finally, the front end process that can present information visually to an end user while allowing control over a bot's configurations and operations.

\section{Information \& Communication Process}
\label{sec:implementation:info_comm}

\noindent As stated in the aims of this project (Sec. \ref{sec:intro:aims}, pg. \pageref{sec:intro:aims}), the core services of \textbf{SKNT} would be delivered using SaaS. The initial design choice to use Flask for the web server was used to implement the communications backbone of the project. Extensions available to Flask allowed for extra functionality to be slotted into the web server. The extensions include Flask-RESTful that allowed the implementation of a HTTP API, Flask-SocketIO that allowed streaming two way communication between the server and client using WebSockets, and lastly Flask-SQLAlchemy that allowed a lightweight SQLite database to store meta data for server management purposes. These extensions are used to create endpoints to retrieve Binance exchange data from, as well as communicate with existing or create new instances of bots.

\subsection{Binance Exchange Data}
\label{sec:implementation:info_comm:binance_exchange_data}

% required to fulfil FR-3, FR-8, NFR-2

% TODO maybe use this in conclusion of impementation?
%\noindent The goal to developing the project's web server was to provide all endpoints for data requests in one place. This meant that, as well as allowing the developed front end UI to communicate with the web server, it is also open and available to all other software that can communicate with HTTP endpoints. 


% TODO include larger discussion of WebSockets as its a main technology in this application as they became more vital after initial research
\noindent During early development, I discovered that modern browsers have security measures against directly requesting data that origins from external domains like \textit{http://api.binance.com}. This prevents a web vulnerability known as \textit{\textbf{Cross Site Scripting}} that while not specifically relevant to this project, was an obstacle during development. A solution to this problem is to proxy the data requests through the web server that would forward the data with the correct configurations; as to let the web browser accept the incoming data that origins from external domains.

WebSockets and HTTP APIs were used to proxy market data through the web server to the UI and the bot's data gathering process (See sec. \ref{sec:implementation:bot}, pg. \pageref{sec:implementation:bot}). The data endpoints that were required and offered by Binance were wrapped by Sam McHardy's \cite{MISC:Python-Binance} Python-Binance wrapper. This allowed for accessing this data through a function call that provided the required arguments to access a coin pair's data. The endpoints that were required provided data that is relevant towards candlestick charts, order books and basic overview data of all coin pairs.

Binance supplies two variations of candlestick chart data, a historical data retrieval through a fetch request and streaming of new data through WebSockets. When designing the requirements it was considered that all data would be gathered solely from the API via fetch request, however as discussed in section \ref{sec:requirements:non_func:eval} (Pg. \pageref{sec:requirements:non_func:eval}) Binance has weight costs to accessing REST endpoints. This raises concerns of maxing out the servers total allowed weight limit within a given interval that would result in an IP ban. Furthermore, a fetch request gathers all historical data within an interval. After the first fetch request, the same data would be received by the web server again except the latest interval potentially being updated. As this is a costly operation, I opted to perform an initial fetch request, then connect to the WebSocket endpoint to stream new candlestick data thereafter, where the activity of operations are dispalyed at figure \ref{fig:implementation:AD_KLINE}. 

\begin{figure}[!htb]
    \centering
	\includegraphics[width=\textwidth]{content/graphics/diagrams/AD_KLINE.png}
	\caption{Activity diagram of candlestick data request}
    
	\label{fig:implementation:AD_KLINE}
\end{figure}

Binance provide documentation of what data format to expect from their endpoints regarding both the HTTP API (App. \ref{sec:appendix:code_snippets}, pg. \pageref{code:impl:info_comm:bin_exch:api_format}) and WebSocket (App. \ref{sec:appendix:code_snippets}, pg. \pageref{code:impl:info_comm:bin_exch:ws_format}) for candlestick data. Interestingly, the data is represented differently at either endpoint with the HTTP endpoint returning a two dimensional array and the WebSocket endpoint continuously streaming a dictionary object. To handle this difference, the web server is required to map the dictionary object into the same format as the two dimensional array to reduce this intensive process away from the client. This ensures consistency for the candlestick data emitted from the web server while reducing front end complexities of handling differently formatted data.

For simplicity, the implemented web sever retrieves the data from both Binance endpoints and emits the data through a single WebSocket connection that the user has made to the web server. As seen in figure \ref{fig:implementation:AD_KLINE}, this is achieved by the web server performing the fetch request to Binance and sending that to the user through it's own WebSocket connection. Then afterwards, starting its own WebSocket connection to Binance, mapping the dictionary to the fetch request format and forwarding the data to the user. 

The streaming of Binance data through their own WebSockets was overlooked during the initial research phase for this project. As such, \textbf{NFR-1} is not as significant as their is no weighting restrictions when using their WebSockets. However, as \textbf{SKNT} - in future iterations - intends to scale to serve multiple users, who on requesting candlestick data require an initial historic data request, an API restriction is used to prevent the web server's IP address from being banned. The API restriction works as the web server performs a \textit{self-ban} for the remainder of the warning interval when receiving a 429 status code from Binance. This status code signifies too many requests have been sent to Binance within the interval on any of their API endpoints.

Referring to figure \ref{fig:implementation:AD_API_RESTRICT}, when the 429 status code is received the status code handler calculates when the next fetch request is allowed to be made. This is done by taking the millisecond timestamp when the status code is received, calculating where the start of the minute interval occurred at and then adding the next minute in milliseconds. This calculation is listed at code snippet \ref{code:impl:exchange_data:calc_next_interval} (Pg. \pageref{code:impl:exchange_data:calc_next_interval}). The calculated time would be added to a SQLite database stored on the web server so the restriction time could be accessible globally. Every API fetch to Binance requires a check of the current time in milliseconds to be greater than the stored time in the database for the request to succeed. %The process in figure \ref{fig:implementation:AD_API_RESTRICT} satisfies NFR-1 and is evaluated in section \ref{sec:evaluation}.

\begin{figure}[!htb]
    \centering
	\includegraphics[width=\textwidth]{content/graphics/diagrams/AD_set_api_ban.png}
	\caption{Activity diagram of self-banning API requests}
	\label{fig:implementation:AD_API_RESTRICT}
\end{figure}

Both the order book and the basic overview data of all coin pairs were also implemented in \textbf{SKNT}. As per the deliverable 1 aims of this project, the order book data endpoint was going to be utilised to find suitable entry into the market, however, due to time constraints these data endpoints are now used as additions to the UX on the front end UI. These endpoints were implemented differently from the candlestick chart data endpoints by having their own dedicated URL for HTTP requests and their own WebSocket endpoints. Their front end use is discussed further in section \ref{sec:implementation:frontend:data_vis} (Pg. \pageref{sec:implementation:frontend:data_vis}). As both endpoints retrieve data from Binance, the API restriction handler and check are utilised. Testing and evaluation of the restriction handler is discussed in section \ref{sec:evaluation:web_server:exch_data} (Pg. \pageref{sec:evaluation:web_server:exch_data}).
% ADD TESTING OF ALL 3 ENDPOINTS TO PROVE API RESTRICTION (will be done in eval and testing section)

\subsection{Bot Communication \& Management}
\label{sec:implementation:info_comm:bot_comm_management}

%TODO start endpoint was originally chosen until I realised there was no easy way to stream the signals to the front end
\noindent There are two HTTP endpoints implemented for controlling and configuring a bot. The first endpoint is used to start a bot by passing the parameters of the coin pair to generate signals from, the ID of the strategy to be used, and the configuration for that strategy. A WebSocket connection can also be used to start the bot as to allow it to stream signals to the front end after initialisation. This WebSocket is required to constantly push the latest signals to the user as they happen. A second REST endpoint is used to stop an existing bot by sending its unique hash ID.

Storing a bot to be communicated with on demand was a challenge during development because they only exist within the web server's memory. There is no straightforward method to storing a reference to a bot in a database that could then be used to communicate with it in memory at a later time. Therefore, a solution to this issue was to create an \textit{existence} class model (Snip. \ref{code:impl:bot:existence_model}, Pg. \pageref{code:impl:bot:existence_model}) that would keep the memory reference of a bot's \textit{manager} class model that can be retrieved by hash IDs. This hash ID is the response returned from the web server when successfully starting a bot providing a way to uniquely identify them in a machine readable way. This model would be able to add or delete bot managers and perform checks to confirm if they exist within the model based on this ID. While this may lead to memory issues when dealing with large quantities of users on the web app's release, this approach is satisfactory for the scope of this project. 

Stopping a bot requires sending the hash ID that is generated on its initialisation as a parameter to the appropriate endpoint. This endpoint queries the \textit{existence} model by performing a look up of the hash ID. A variety of checks are performed to return the correct status code, such as if it doesn't exist.  This would illicit an appropriate response so that the UI can handle this correctly. If the bot does exist, then the bot is stopped and a further check is performed to see if it is stopped successfully. These checks are crucial to ensuring a robust system that confirms the existence of a bot. This approach is common throughout the system and allows multiple variations of responses to be returned. This level of varied responses keeps the user updated about the outcome of requests.
%TODO create diagram like the handshake protocol to show communication to the web server. MAYBE, already got a lot of diagrams

The bot's \textit{manager} class model is the hub that a user's interactions affect when starting and stopping the bot. This model manages one bot through it's three \textit{actors} which are discussed in the bot process (Sec. \ref{sec:implementation:bot}) and ensures it is initialised with required parameters correctly, such as a valid user WebSocket session ID to forward the output to, the ID of the strategy, and it's configuration options. This model also performs checks that bot is properly shutdown by ensuring the three actors have been terminated which is crucial to preventing memory leaks and wasted resources of a background task running unbeknownst. Designing this model allows for multiple bots to operate concurrently which prepares for future work on multiple simultaneous users.


\section{Bot Process}
\label{sec:implementation:bot}

\noindent As mentioned above, the bot process is built on \textit{actor} models for a non-blocking concurrent data flow. This decision was made due the initial design of the system processing operations sequentially. During development the system would block until the blocking intensive task was completed. This raised issues, such as the gathering of Binance data missing updates as the intensive process of generating signals was being executed. This meant crucial data could be backlogged or potentially missed and resulted in poor performance of \textbf{SKNT} as a whole.

% socketio client, listens to existing namespace, gets intial data then receives updates thereafter
The first stage of the process was to gather the exchange data to operate the bot. A WebSocket client would connect and listen to the web server's endpoint for candlestick data as it was the simplest and cleanest solution. This is because the client listens to the already implemented endpoint that pushes the data to the front end UI, which is discussed in section \ref{sec:implementation:info_comm:binance_exchange_data} (Pg. \pageref{sec:implementation:info_comm:binance_exchange_data}). This ensured consistency between the data present on the front end UI and what the bot was processing, as well as reducing the required redundant boilerplate code and testing. As the WebSocket client library \cite{MISC:socketio_client} handles incoming data concurrently, the actor model is not required at this stage. The data gatherer actor simply listens for new incoming data and forwards it to the processing actor.

The actor model used for this process is based on an \textit{inbox} system. This is a queue that actors constantly check for new \textit{messages}. When a message is received it is popped from the front of the queue and handled appropriately. This allows for other parts of the program to execute while the actor is waiting on messages. This couples the different processes of the system by the data flow being pushed to each actor in a non-blocking way as seen in figure \ref{fig:implementation:actor_dataflow} (Pg. \pageref{fig:implementation:actor_dataflow}). 

\begin{figure}[htb]
    \centering
	\includegraphics[width=0.9\textwidth]{content/graphics/diagrams/actor_dataflow.png}
	\caption{Data flow of the bot process: 
	\textit{(a)} The web server emits candlestick data to WebSocket client
	\textit{(b)} WebSocket client forwards data to actor 1's inbox
    \textit{(c)} Actor 1 pops data from inbox and parses data into pandas data structure
    \textit{(d)} Actor 1 executes chosen strategy using BackTrader and pushes signals to actor 2
    \textit{(e)} Actor 2 pops data from inbox passes to handler
    \textit{(f)} Actor 2 emits data to specific client}
    
	\label{fig:implementation:actor_dataflow}
\end{figure}

% data process, strategies, keeps data in memory, only updates
%maybe discuss how backtrader limitations of supporting live data from their selected brokers required work arounds?
\noindent The data processing stage is an actor and the most intensive part of the bot process. Shown in figure \ref{fig:implementation:actor_dataflow}.c, the actor receives candlestick data into its inbox. This is received as a standard python list and requires to be transformed into a pandas DataFrame. The DataFrame data structure is required as it can index and order itself by time stamp. This is extremely beneficial for manipulating time series data and why it is useful for financial software. This initial DataFrame is stored in the actor and all the following data appends or updates the latest interval of the DataFrame. 

The Back Trader library is utilised in the following steps of the data processor as shown in figure \ref{fig:implementation:actor_dataflow}.d. The strategy and its parameters that were given to the manager model are executed on the pandas DataFrame. Back Trader applies the strategy on the data and generates signals when signal conditions are met. Back Trader does present some limitations, such as being unable to retrieve signals from its internal data structures after executing the strategy. There is no clear method presented in the documentation to retrieve the signals generated from their engine. This issue required creating a \textit{workaround}, by appending each signal to my own internal array and attaching it to the result returned from executing the strategy. This method is \textit{hacky} at best and requires signals to be parsed by a custom mapping.


The strategies used for \textbf{SKNT} are designed using the technical indicators discussed in section \ref{sec:related:tradingStrategies} (Pg. \pageref{sec:related:tradingStrategies}), such as the \textbf{SMA}, \textbf{EMA}, \textbf{RSI}, \textbf{MACD} and \textbf{Bollinger Bands}. The indicators are provided by the Back Trader library \cite{MISC:BACKTRADER} and can be easily imported into the strategy. Back Trader performs signal generation by the repeated execution of the \textbf{next()} function on each interval of data which checks conditions of the indicators. In the case of the \textbf{SMA Crossover \& RSI} strategy, a buy signal occurs when the \textbf{RSI} is below it's lower threshold (default is 30) and when the smaller \textbf{SMA} interval (default is 15) crosses above the longer \textbf{SMA} interval (default is 30) that is shown at code snippet \ref{code:impl:bot_process:sma_rsi} (App. \ref{sec:appendix:code_snippets}, pg. \pageref{code:impl:bot_process:sma_rsi}). Defining a strategy is relatively straightforward with Back Trader as manipulation and conditional checks can be used like other natural programming constructs.

Handling the variation on the number of parameter options between strategies required a standard between the data processor, the web server's bot communicator and the front end UI. This is to ensure that the inputs of a parameter entered on the UI correspond with the correct parameter in the strategy. To tackle this, dictionary objects (Snip. \ref{code:implementation:bot_process:strat_option_data}) were used to look up the parameter's value by a key. For the scope of this project, the keys were string versions of integers from \textbf{0} to \textbf{N} (\textbf{N} being the number of parameters for the signal). Using a dictionary allows the keys to be changed appropriately when the complexities of strategies are increased, improving on maintainability for future iterations of the project.

\begin{listing}
\caption{Default configuration data for the "MA Crossover \& RSI" strategy} 
\label{code:implementation:bot_process:strat_option_data}
\begin{minted}[linenos=true,
               xleftmargin=21pt,
               tabsize=2]{js}
{   
    0: 15,      // MA Small Period
    1: 30,      // MA Large Period
    2: 70,      // RSI Upper Threshold
    3: 30,      // RSI Lower Threshold
    4: "SMA"    // MA Type (SMA or EMA)
}
\end{minted}
\end{listing}

% signal handling process, emit to correct user
% TODO maybe showcase websocket SID is valid and how this actor handles that
The last actor used in the bot process is the signal processor seen in figure \ref{fig:implementation:actor_dataflow}.e and \ref{fig:implementation:actor_dataflow}.f. The bot manager initialises this actor with the user's WebSocket session ID so the actor can emit signals to the correct users. This actor is designed to forward data to the correct user and validate that there is a user to receive the signals. It tackles this problem by confirming that there is an SID and there is an active connection to the user. 

Overall, the actor model is beneficial towards the development of \textbf{SKNT} by providing a non-blocking flow of data between the main components of the bot shown by figure \ref{fig:implementation:actor_dataflow}. This model circumvents the intensive signal generation process that the data processor requires, while being able to continually listen for new data being received from the web server. Strategies can be easily slotted into the data processor by modular design of the data processor. 



\section{Front End Process}
\label{sec:implementation:frontend}

% TODO add react and redux to lit review
\noindent Implementation of the front end can be categorised into two main sections, data visualisation of Binance's market data, and user controls and notifications of the platform. Both sections utilise two main technologies, React which is a UI library and Redux which is an application state manager. Both technologies integrate well together, with React creating self-contained components and Redux tying them together with a central \textit{store}. Redux was necessary to allow actions of controllers update the rest of the UI. For example, selecting a different coin pair such as \textbf{ETH / USDT} would trigger an action to update the store's state to the newly selected coin pair. This would then cause the candlestick chart, order book, quick information panel and bot controller options to update. 

% Data Visualisation of Market Data Section
\subsection{Data Visualisation of Market Data}
\label{sec:implementation:frontend:data_vis}
%% Candlestick/Volume
\noindent The main area of the front end UI is the candlestick chart. This chart can visualise current and historic price information of a coin pair. It displays information on the open and close and highest and lowest prices within an interval. The chart itself is a library that is heavily customisable and requires data to be passed into it. The logic for data retrieval is implemented in a component that renders the chart. When the component is loaded (mounted), a WebSocket is opened to the web server to request data. This request takes the current coin pair saved in the Redux state - \textbf{BTC/USDT} by default - and requests the streaming of data. The component then \textit{listens} for the response which is then parsed and pushed into the candlestick chart component which populates the intervals.

% Maybe code snippet of mapping function and data retrieved?
Parsing the retrieved data uses a mapping function to transform a list into a JavaScript object. The millisecond timestamp is converted to a UTC string date\footnote{e.g. Fri, 02 Feb 1996 03:04:05 GMT} and assigned to a key-value pair in a JavaScript object, along with the open, close, highest, lowest and volume data indexes in the list. Performance may suffer on weak systems when large amounts of data is required to be parsed to this format. While performance issues did not occur on development systems, this may be worth further investigating in future iterations. One solution to increasing efficiency could be to preprocess the required format on the server before the user retrieves the data. This mapping operation could also integrate with the caching of market data for \textbf{NFR-6} that is discussed in section \ref{sec:requirements:non_func} to then only have to execute this operation once. This will be discussed for future areas of work as it was not achieved during this project.

Unfortunately, due to time constraints, implementation of technical indicators corresponding to the indicators a strategy would use was missed within the time line of this project. This could be implemented in future iterations, however, the \textbf{SMA} and \textbf{Bollinger Band} indicators are displayed on the chart as placeholders. These are provided by the charting library which support a vast number of technical indicators and display this can be implemented with ease. Further developing the UI to align with the bot's strategy would offer the user more information of how the bot derived signals from it's indicators.

% TODO conclude on Redux making the management of React components simple and gives better control over the web app
%% Orderbook
Within the bot's signal generation process, the order book is no longer used to perform risk analysis of the market for trade executions due to the revision of the initial aims of the project. However, the UI implementation brings information about the market's depth to the user and can display how liquid or illiquid a market is. Developing the order book had challenges to ensure the data displayed is accurate with no missing pieces. This firstly required streaming data from the WebSocket into a buffer that holds data representing \textit{updates} to the current order book. After receiving the first payload from the WebSocket, a fetch request for the order book's historical data is sent. After receiving the response data, the streamed data residing in the buffer with a timestamp older (smaller) than the historical data's timestamp is discarded. The historic data is then placed in the buy or sell order book table and the rest of the order book buffer is emptied to update the tables. All new streamed data is then able to update these tables, and ensures the buy and sell order books are accurate and up-to-date.

%% Quick Info Panel
For a quick overview of the market conditions, a data panel is provided at the top of the UI providing information such as the current coin price, what coin pair is currently being viewed, the volume traded within the last 24 hours and other useful information. This panel is managed entirely by the Redux state manager and is the main point of reference to feedback meta information the user is currently looking at. All the data is collected and updated from different components of the UI and pushed into Redux, which then pushes this data into the quick view panel to display the aggregated data.

% User Controls and Notifications
\subsection{User Controls and Notifications}
\label{sec:implementation:frontend:controls_notifications}
%% Bot Controls
% TODO pretty weak intro, fix it
\noindent The bot control panel has two main points of functionality, toggling the bot on or off and configuring the bot through strategy selection. By implementing the strategies and their configurations directly into the front end has increased difficulty for future maintainability and addition of strategies. As strategies are written directly into the Redux state, updating or adding a strategy requires that they are also updated on the web server. This means that updates to the strategies require change in two different parts of the application. In future work, implementing an endpoint that the front end can request all offered strategies and their configurations would reduce the time required to add or update strategies.

The strategy selection and configuration panel provides a drop down box of strategies that are currently offered. Selecting a strategy displays parameters specific to that strategy and which the user may customise. These configuration options are sent to the server when starting the bot. After requesting a bot to start, the bot's hash ID is returned as the initial response. This hash ID is stored on the front end to allow the user to stop the bot at a later point. Their is limitations towards the current implementation as it can only store one hash ID and only during the current browser session. This provides limitations towards future goals of controlling multiple bots simultaneously. One solution would be to create a login and account management system where a bots hash IDs can be attached to an account and stored in a database. This would create persistence of controlling the bot between web browser sessions by being able to retrieve the hash IDs on login and allowing multiple bot IDs to be controlled. 

%% Push Message Panel
Keeping the user up-to-date with the current status and operations of the bot led to implementing two types of notifications. The first being a push message panel that creates a log of vital system messages that can communicate the current status and intent of a bot. The second being \textit{snackbar} notifications that provide system confirmations for convenience, providing feedback to user interactions to confirm they have been processed. The snackbar implementation is provided by material-ui React library and offers colour coded message types. This offers different information to the user through success, warning, error and information messages. Both types of messages can be generated by both the front end and the web server that provides flexibility to communicating events to the user.



%% Coin pair selection
The coin pair selection panel provides greater choice to the user and was developed by feeding data into a table. The rows in the table represent all the coin pairs available on Binance that can be selected. A selection would update the store's state of the web app to clear all current data on the candlestick, order book and coin info panel. An update would be sent to the web server to inform the user's selection of coin pair has changed and that different market data was required. The biggest challenge for this functionality was updating the different components of the system to reflect this choice, but integrating this with Redux updated the entire state of the UI with a single function call. The web server used meta information to reduce resources being used by closing WebSocket streams of coin pair's no longer being requested. This was done by tracking the number of user's WebSocket session IDs that had requested data for a specific coin and updated their choice when they selected another coin pair.

%% Page Selection???






